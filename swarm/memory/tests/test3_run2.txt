Based on your recent decisions shown in the memory context, you've already settled on **E5-large-v2** for local embeddings (mentioned multiple times), with **nomic-embed-text** as an alternative, and **text-embedding-3-large** for API calls.

Here's a quick summary of what you've decided:

| Use Case | Model | Notes |
|----------|-------|-------|
| **Local** | E5-large-v2 | Primary choice, good quality/speed balance |
| **Local alt** | nomic-embed-text | Alternative option |
| **API** | text-embedding-3-large | OpenAI's latest |

**E5-large-v2 specifics:**
- 1024-dim embeddings
- ~335M parameters
- Prefix queries with `"query: "` and passages with `"passage: "` for best results
- Works well with your 320-token chunks

Is there something specific about embeddings you want to explore further, or were you checking what was already decided?
