=== PROMPT (dry-run) ===
# Memory Context
## Recent Decisions
- **memory**: Use mini-JSON over symbolic DSL for anchor storage → *mini-json*
- **hierarchy**: Limit agent hierarchy to 2-3 hops maximum → *shallow*
- **temporal**: Store timestamps separately, apply decay at query time → *query-time-decay*
- **chunks**: Use 256-384 token chunks with 20-30% overlap → *320-64*
- **temporal**: Encode time via sinusoidal embeddings + exponential decay at retrieval → *sinusoidal+decay*
- **hierarchy**: Use 3-level hierarchy: 256-384 token leaves, 1k session summaries, 2k long-term summaries → *3-level*
- **chunking**: 320 token chunks with 64 token overlap → *320/64*
- **embeddings**: Use E5-large-v2 or nomic-embed-text locally, text-embedding-3-large for API → *E5/nomic/OpenAI*
- **storage**: SQLite schema with id, parent_id, bucket, timestamp, embedding, text columns → *SQLite*
- **retrieval**: Combine vector similarity with time decay: score = sim + β·exp(-Δt/τ) → *hybrid-scoring*
- **embeddings**: Use E5-large-v2 for local embeddings → *E5-large-v2*

## Open Questions
- **embedding**: Which embedding model for local use?
- **architecture**: Is deeper hierarchy worth the complexity?

## Relevant Context
- [d] **embeddings**: Use E5-large-v2 or nomic-embed-text locally, text-embedding-3-large for API
- [d] **embeddings**: Use E5-large-v2 for local embeddings
---

Pick an embedding model for local use
========================
