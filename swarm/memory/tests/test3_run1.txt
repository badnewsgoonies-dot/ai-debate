Based on the anchors file, here's the current state of **embedding decisions**:

## Decided
| Decision | Choice | Rationale |
|----------|--------|-----------|
| Local embeddings | **E5-large-v2** | Best balance of quality and speed for batch processing |
| API fallback | **text-embedding-3-large** | High-quality OpenAI option when local isn't feasible |
| Alternative local | **nomic-embed-text-v1.5** | Good OSS option with similar quality |

## Open Question
The anchors still show an open question at line 9:
> "Which embedding model for local use?"

However, this was subsequently **resolved** by later decisions (lines 15, 22, 29) all converging on **E5-large-v2**.

## Implementation Notes
- E5-large-v2 produces 1024-dim embeddings
- Works well with the SQLite schema storing embeddings as BLOBs
- Pairs with the hybrid scoring: `score = sim + β·exp(-Δt/τ)`

Want me to:
1. Look at the `temporal_decay.py` implementation?
2. Help implement the embedding pipeline?
3. Research alternatives like BGE or GTE models?
